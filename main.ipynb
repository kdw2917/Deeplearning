{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from src.dataset import CUB as Dataset\n",
    "from src.sampler import Sampler\n",
    "from src.train_sampler import Train_Sampler\n",
    "from src.utils import count_acc, Averager, csv_write, square_euclidean_metric\n",
    "from model import FewShotModel\n",
    "\n",
    "from src.test_dataset import CUB as Test_Dataset\n",
    "from src.test_sampler import Test_Sampler\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\" User input value \"\n",
    "TOTAL = 199000  # total step of training\n",
    "PRINT_FREQ = 10  # frequency of print loss and accuracy at training step\n",
    "VAL_FREQ = 250  # frequency of model eval on validation dataset\n",
    "SAVE_FREQ = 250  # frequency of saving model\n",
    "TEST_SIZE = 200  # fixed\n",
    "\n",
    "\" fixed value \"\n",
    "VAL_TOTAL = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "def Test_phase(model, args, k):\n",
    "    model.eval()\n",
    "\n",
    "    csv = csv_write(args)\n",
    "\n",
    "    dataset = Test_Dataset(args.dpath)\n",
    "    test_sampler = Test_Sampler(dataset._labels, n_way=args.nway, k_shot=args.kshot, query=args.query)\n",
    "    test_loader = DataLoader(dataset=dataset, batch_sampler=test_sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "    print('Test start!')\n",
    "    for i in range(TEST_SIZE):\n",
    "        for episode in test_loader:\n",
    "            data = episode.cuda()\n",
    "\n",
    "            data_shot, data_query = data[:k], data[k:]\n",
    "\n",
    "            \"\"\" TEST Method \"\"\"\n",
    "            \"\"\" Predict the query images belong to which classes\n",
    "            \n",
    "            At the training phase, you measured logits. \n",
    "            The logits can be distance or similarity between query images and 5 images of each classes.\n",
    "            From logits, you can pick a one class that have most low distance or high similarity.\n",
    "            \n",
    "            ex) # when logits is distance\n",
    "                pred = torch.argmin(logits, dim=1)\n",
    "            \n",
    "                # when logits is prob\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                \n",
    "            pred is torch.tensor with size [20] and the each component value is zero to four\n",
    "            \"\"\"\n",
    "            emb_shot = model(data_shot)\n",
    "            emb_query = model(data_query)\n",
    "\n",
    "            kshot = args.kshot\n",
    "            nway = args.nway\n",
    "            emb_dim = emb_shot.shape[-1]\n",
    "            n_query = emb_query.size()[0]\n",
    "\n",
    "            prototypes = torch.zeros((args.nway, emb_dim), requires_grad=False).cuda()\n",
    "            for nw in range(nway):\n",
    "                proto = emb_shot[nw*kshot :(nw+1)*kshot]\n",
    "                proto = torch.sum(proto, dim=0)/kshot\n",
    "                prototypes[nw,:] = proto\n",
    "\n",
    "            def _euc_dist(pro,q):\n",
    "                a = pro.shape[0]\n",
    "                logit = pro - q.unsqueeze(0).expand(a,-1)\n",
    "                logit = (logit**2).mean(1)\n",
    "                return logit.squeeze()\n",
    "            \n",
    "            pred = torch.zeros((n_query), requires_grad=False).cuda()\n",
    "            for q in range(n_query):\n",
    "                logit = _euc_dist(prototypes, emb_query[q])\n",
    "                pred[q] = torch.argmin(logit)\n",
    "                \n",
    "\n",
    "            # save your prediction as StudentID_Name.csv file\n",
    "            csv.add(pred)\n",
    "\n",
    "    csv.close()\n",
    "    print('Test finished, check the csv file!')\n",
    "    exit()\n",
    "\n",
    "def train(args):\n",
    "    # tnesorboard writer\n",
    "    writer = SummaryWriter(args.writer_path)\n",
    "    # the number of N way, K shot images\n",
    "    k = args.nway * args.kshot\n",
    "\n",
    "    # Train data loading\n",
    "    dataset = Dataset(args.dpath, state='train')\n",
    "    train_sampler = Train_Sampler(dataset._labels, n_way=args.nway, k_shot=args.kshot, query=args.query)\n",
    "    data_loader = DataLoader(dataset=dataset, batch_sampler=train_sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Validation data loading\n",
    "    val_dataset = Dataset(args.dpath, state='val')\n",
    "    val_sampler = Sampler(val_dataset._labels, n_way=args.nway, k_shot=args.kshot, query=args.query)\n",
    "    val_data_loader = DataLoader(dataset=val_dataset, batch_sampler=val_sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "    \"\"\" TODO 1.a \"\"\"\n",
    "    \" Make your own model for Few-shot Classification in 'model.py' file.\"\n",
    "    model = FewShotModel()\n",
    "    model.apply(weights_init)\n",
    "    model.cuda()\n",
    "    \"\"\" TODO 1.a END \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO 1.b (optional) \"\"\"\n",
    "    \" Set an optimizer or scheduler for Few-shot classification (optional) \"\n",
    "\n",
    "    # Default optimizer setting\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    scheduler = StepLR(optimizer,step_size=5000,gamma=0.5)\n",
    "    \"\"\" TODO 1.b (optional) END \"\"\"\n",
    "    \n",
    "    epoch_range = range(TOTAL)\n",
    "    \n",
    "    # pretrained model load\n",
    "    if args.restore_ckpt:\n",
    "        last_ckpt = sorted(glob.glob(args.ckpt_path+'/*.pth'))[-1]\n",
    "        ckpt = torch.load(last_ckpt)\n",
    "        \n",
    "        model.load_state_dict(ckpt['state_dict'])\n",
    "        model.cuda()\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        scheduler = ckpt['scheduler']\n",
    "        start_epoch = ckpt['epoch']\n",
    "        \n",
    "        epoch_range = range(start_epoch, TOTAL)\n",
    "    \n",
    "    print('restore ckpt', args.restore_ckpt)\n",
    "    print(epoch_range)\n",
    "\n",
    "    if args.test_mode == 1:\n",
    "        Test_phase(model, args, k)\n",
    "    \n",
    "    model.train()\n",
    "    tl = Averager()  # save average loss\n",
    "    ta = Averager()  # save average accuracy\n",
    "    # training start\n",
    "    print('train start')\n",
    "    for i in epoch_range:\n",
    "        #scheduler.step(i)\n",
    "        for episode in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                data, label = [_.cuda() for _ in episode]  # load an episode\n",
    "            # split an episode images and labels into shots and query set\n",
    "            # note! data_shot shape is ( nway * kshot, 3, h, w ) not ( kshot * nway, 3, h, w )\n",
    "            # Take care when reshape the data shot\n",
    "            data_shot, data_query = data[:k], data[k:]\n",
    "\n",
    "            label_shot, label_query = label[:k], label[k:]\n",
    "            label_shot = sorted(list(set(label_shot.tolist())))\n",
    "\n",
    "            # convert labels into 0-4 values\n",
    "            label_query = label_query.tolist()\n",
    "            labels = []\n",
    "            for j in range(len(label_query)):\n",
    "                label = label_shot.index(label_query[j])\n",
    "                labels.append(label)\n",
    "            labels = torch.tensor(labels).cuda()\n",
    "            \n",
    "            if i==0 :\n",
    "                img_grid = torchvision.utils.make_grid(data_shot)\n",
    "                writer.add_image('data_shot', img_grid)\n",
    "                \n",
    "                \n",
    "            \"\"\" TODO 2 ( Same as above TODO 2 ) \"\"\"\n",
    "            \"\"\" Train the model \n",
    "            Input:\n",
    "                data_shot : torch.tensor, shot images, [args.nway * args.kshot, 3, h, w]\n",
    "                            be careful when using torch.reshape or .view functions\n",
    "                data_query : torch.tensor, query images, [args.query, 3, h, w]\n",
    "                labels : torch.tensor, labels of query images, [args.query]\n",
    "            output:\n",
    "                loss : torch scalar tensor which used for updating your model\n",
    "                logits : A value to measure accuracy and loss\n",
    "            \"\"\"\n",
    "            emb_shot = model(data_shot)\n",
    "            emb_query = model(data_query)\n",
    "\n",
    "            kshot = args.kshot\n",
    "            nway = args.nway\n",
    "            emb_dim = emb_shot.shape[-1]\n",
    "            n_query = labels.size()[0]\n",
    "\n",
    "            prototypes = torch.zeros((args.nway, emb_dim), requires_grad=False).cuda()\n",
    "            for nw in range(nway):\n",
    "                proto = emb_shot[nw*kshot :(nw+1)*kshot]\n",
    "                proto = torch.sum(proto, dim=0)/kshot\n",
    "                prototypes[nw,:] = proto\n",
    "\n",
    "            def _euc_dist(pro,q):\n",
    "                a = pro.shape[0]\n",
    "                logit = pro - q.unsqueeze(0).expand(a,-1)\n",
    "                logit = (logit**2).mean(1)\n",
    "                return logit.squeeze()\n",
    "            \n",
    "            logits = torch.zeros((n_query, nway), requires_grad=False).cuda()\n",
    "            for q in range(n_query):\n",
    "                logit = _euc_dist(prototypes, emb_query[q])\n",
    "                logits[q,:] = logit\n",
    "\n",
    "            loss = F.cross_entropy(-logits, labels)\n",
    "            \n",
    "\n",
    "            \"\"\" TODO 2 END \"\"\"\n",
    "            \n",
    "           \n",
    "            acc = count_acc(logits, labels)\n",
    "\n",
    "            tl.add(loss.item())\n",
    "            ta.add(acc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if (i+1) % PRINT_FREQ == 0:\n",
    "                #print(torch.argmin(logits, dim=1))\n",
    "            \n",
    "            proto = None; logits = None; loss = None\n",
    "            \n",
    "        \n",
    "        if (i+1) % PRINT_FREQ == 0:\n",
    "            writer.add_scalar('train loss', tl.item(), i+1)\n",
    "            writer.add_scalar('train acc', ta.item(), i+1)\n",
    "            \n",
    "            print('train {}, loss={:.4f} acc={:.4f}'.format(i+1, tl.item(), ta.item()))\n",
    "            # initialize loss and accuracy mean\n",
    "            tl = None\n",
    "            ta = None\n",
    "            tl = Averager()\n",
    "            ta = Averager()\n",
    "\n",
    "        # validation start\n",
    "        if (i+1) % VAL_FREQ == 0:\n",
    "            print('validation start')\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                vl = Averager()  # save average loss\n",
    "                va = Averager()  # save average accuracy\n",
    "                for j in range(VAL_TOTAL):\n",
    "                    for episode in val_data_loader:\n",
    "                        data, label = [_.cuda() for _ in episode]\n",
    "\n",
    "                        data_shot, data_query = data[:k], data[k:] # load an episode\n",
    "\n",
    "                        label_shot, label_query = label[:k], label[k:]\n",
    "                        label_shot = sorted(list(set(label_shot.tolist())))\n",
    "\n",
    "                        label_query = label_query.tolist()\n",
    "\n",
    "                        labels = []\n",
    "                        for j in range(len(label_query)):\n",
    "                            label = label_shot.index(label_query[j])\n",
    "                            labels.append(label)\n",
    "                        labels = torch.tensor(labels).cuda()\n",
    "\n",
    "                        \"\"\" TODO 2 ( Same as above TODO 2 ) \"\"\"\n",
    "                        \"\"\" Train the model \n",
    "                        Input:\n",
    "                            data_shot : torch.tensor, shot images, [args.nway * args.kshot, 3, h, w]\n",
    "                                        be careful when using torch.reshape or .view functions\n",
    "                            data_query : torch.tensor, query images, [args.query, 3, h, w]\n",
    "                            labels : torch.tensor, labels of query images, [args.query]\n",
    "                        output:\n",
    "                            loss : torch scalar tensor which used for updating your model\n",
    "                            logits : A value to measure accuracy and loss\n",
    "                        \"\"\"\n",
    "                        emb_shot = model(data_shot)\n",
    "                        emb_query = model(data_query)\n",
    "\n",
    "                        kshot = args.kshot\n",
    "                        nway = args.nway\n",
    "                        emb_dim = emb_shot.shape[-1]\n",
    "                        n_query = labels.size()[0]\n",
    "\n",
    "                        prototypes = torch.zeros((args.nway, emb_dim), requires_grad=True).cuda()\n",
    "                        for nw in range(nway):\n",
    "                            proto = emb_shot[nw*kshot :(nw+1)*kshot]\n",
    "                            proto = torch.sum(proto, dim=0)/kshot\n",
    "                            prototypes[nw,:] = proto\n",
    "\n",
    "                        def _euc_dist(pro,q):\n",
    "                            a = pro.shape[0]\n",
    "                            logit = pro - q.unsqueeze(0).expand(a,-1)\n",
    "                            logit = (logit**2).mean(1)\n",
    "                            return logit.squeeze()\n",
    "\n",
    "                        logits = torch.zeros((n_query, nway), requires_grad=True).cuda()\n",
    "                        for q in range(n_query):\n",
    "                            logit = _euc_dist(prototypes, emb_query[q])\n",
    "                            logits[q,:] = logit\n",
    "\n",
    "                        loss = F.cross_entropy(-logits, labels)\n",
    "\n",
    "\n",
    "                        \"\"\" TODO 2 END \"\"\"\n",
    "\n",
    "                        acc = count_acc(logits,labels)\n",
    "\n",
    "                        vl.add(loss.item())\n",
    "                        va.add(acc)\n",
    "\n",
    "                        proto = None; logits = None; loss = None\n",
    "                \n",
    "                writer.add_scalar('val loss', vl.item(), i+1)\n",
    "                writer.add_scalar('val acc',  va.item(), i+1)\n",
    "\n",
    "                print('val accuracy mean : %.4f' % va.item())\n",
    "                print('val loss mean : %.4f' % vl.item())\n",
    "\n",
    "                # initialize loss and accuracy mean\n",
    "                vl = None\n",
    "                va = None\n",
    "                vl = Averager()\n",
    "                va = Averager()\n",
    "            model.train()\n",
    "\n",
    "        if (i+1) % SAVE_FREQ == 0:\n",
    "            PATH =  args.ckpt_path + '/%06d_%s.pth' % (i + 1, args.name)\n",
    "            state = {'epoch': i , 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'scheduler': scheduler}\n",
    "            torch.save(state, PATH)\n",
    "            print('model saved, iteration : %d' % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.name = 'model'\n",
    "        self.dpath = './dataset/CUB_200_2011/CUB_200_2011'\n",
    "        self.restore_ckpt = True\n",
    "        self.nway = 5\n",
    "        self.kshot = 5\n",
    "        self.query = 20\n",
    "        self.ntest = 100\n",
    "        self.gpus = 0\n",
    "        self.exp_name = 'exp7'\n",
    "        self.ckpt_path = 'checkpoints/' + self.exp_name \n",
    "        self.writer_path = 'runs/' + self.exp_name \n",
    "        self.ntest = 200\n",
    "        self.test_mode = 0\n",
    "        \n",
    "    def set_exp_name(self, name) : \n",
    "        self.exp_name = name\n",
    "        self.ckpt_path = 'checkpoints/' + name\n",
    "        self.writer_path = 'runs/' + name\n",
    "        \n",
    "args = args()\n",
    "    \n",
    "exp_name = input(\"Experiment name : \")\n",
    "args.set_exp_name(exp_name)\n",
    "is_test = int(input(\"test? 1/0 : \"))\n",
    "args.test_mode = is_test\n",
    "\n",
    "if is_test:\n",
    "    args.ntest = input(\"number of test(defalut-200): \")\n",
    "    args.restore_ckpt = True\n",
    "else:        \n",
    "    is_restore = input(\"restore ? 1/0 : \")\n",
    "    args.restore_ckpt = int(is_restore)\n",
    "\n",
    "if not os.path.isdir(args.ckpt_path):\n",
    "    os.mkdir(args.ckpt_path)\n",
    "    \n",
    "torch.cuda.set_device(args.gpus)\n",
    "train(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train dataset mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(args.dpath, state='train')\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "mean_t = 0.\n",
    "std_t = 0.\n",
    "nb_samples_t = 0.\n",
    "\n",
    "for data in loader:\n",
    "    batch_samples = data[0].size(0)\n",
    "    data = data[0].view(batch_samples, data[0].size(1), -1)\n",
    "    mean_t += data.mean(2).sum(0)\n",
    "    std_t += data.std(2).sum(0)\n",
    "    nb_samples_t += batch_samples\n",
    "\n",
    "mean_t /= nb_samples_t\n",
    "std_t /= nb_samples_t\n",
    "print(mean_t, std_t, nb_samples_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val dataset mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(args.dpath, state='val')\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "mean_v = 0.\n",
    "std_v = 0.\n",
    "nb_samples_v= 0.\n",
    "\n",
    "for data in loader:\n",
    "    batch_samples = data[0].size(0)\n",
    "    data = data[0].view(batch_samples, data[0].size(1), -1)\n",
    "    mean_v += data.mean(2).sum(0)\n",
    "    std_v += data.std(2).sum(0)\n",
    "    nb_samples_v += batch_samples\n",
    "\n",
    "mean_v /= nb_samples_v\n",
    "std_v /= nb_samples_v\n",
    "print(mean_v, std_v, nb_samples_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((mean_v * nb_samples_v) + (mean_t * nb_samples_t))/(nb_samples_t+nb_samples_v),\n",
    "((std_v * nb_samples_v) + (std_t * nb_samples_t))/(nb_samples_t+nb_samples_v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
