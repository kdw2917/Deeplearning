{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from src.dataset import CUB as Dataset\n",
    "from src.sampler import Sampler\n",
    "from src.train_sampler import Train_Sampler\n",
    "from src.utils import count_acc, Averager\n",
    "from model import FewShotModel\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\" User input value \"\n",
    "TOTAL = 99000  # total step of training\n",
    "PRINT_FREQ = 10  # frequency of print loss and accuracy at training step\n",
    "VAL_FREQ = 250  # frequency of model eval on validation dataset\n",
    "SAVE_FREQ = 250  # frequency of saving model\n",
    "TEST_SIZE = 200  # fixed\n",
    "\n",
    "\" fixed value \"\n",
    "VAL_TOTAL = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0, 0.01)\n",
    "        torch.nn.init.ones_(m.bias)\n",
    "        \n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # tnesorboard writer\n",
    "    writer = SummaryWriter(args.writer_path)\n",
    "    \n",
    "    # the number of N way, K shot images\n",
    "    k = args.nway * args.kshot\n",
    "\n",
    "    # Train data loading\n",
    "    dataset = Dataset(args.dpath, state='train')\n",
    "    train_sampler = Train_Sampler(dataset._labels, n_way=args.nway, k_shot=args.kshot, query=args.query)\n",
    "    data_loader = DataLoader(dataset=dataset, batch_sampler=train_sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Validation data loading\n",
    "    val_dataset = Dataset(args.dpath, state='val')\n",
    "    val_sampler = Sampler(val_dataset._labels, n_way=args.nway, k_shot=args.kshot, query=args.query)\n",
    "    val_data_loader = DataLoader(dataset=val_dataset, batch_sampler=val_sampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "    \"\"\" TODO 1.a \"\"\"\n",
    "    \" Make your own model for Few-shot Classification in 'model.py' file.\"\n",
    "    model = FewShotModel()\n",
    "    model.apply(weights_init)\n",
    "    model.cuda()\n",
    "    \"\"\" TODO 1.a END \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO 1.b (optional) \"\"\"\n",
    "    \" Set an optimizer or scheduler for Few-shot classification (optional) \"\n",
    "\n",
    "    # Default optimizer setting\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    scheduler = StepLR(optimizer,step_size=5000,gamma=0.5)\n",
    "    \"\"\" TODO 1.b (optional) END \"\"\"\n",
    "    \n",
    "    epoch_range = range(TOTAL)\n",
    "    \n",
    "    # pretrained model load\n",
    "    if args.restore_ckpt:\n",
    "        last_ckpt = sorted(glob.glob(args.ckpt_path+'/*.pth'))[-1]\n",
    "        ckpt = torch.load(last_ckpt)\n",
    "        \n",
    "        model.load_state_dict(ckpt['state_dict'])\n",
    "        model.cuda()\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        scheduler = ckpt['scheduler']\n",
    "        start_epoch = ckpt['epoch']\n",
    "        \n",
    "        epoch_range = range(start_epoch, TOTAL)\n",
    "    \n",
    "       \n",
    "    print('restore ckpt', args.restore_ckpt)\n",
    "    print('start epoch: ', start_epoch)\n",
    "    \n",
    "       \n",
    "    model.train()\n",
    "    tl = Averager()  # save average loss\n",
    "    ta = Averager()  # save average accuracy\n",
    "    # training start\n",
    "    print('train start')\n",
    "    for i in epoch_range:\n",
    "        #scheduler.step(i)\n",
    "        for episode in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                data, label = [_.cuda() for _ in episode]  # load an episode\n",
    "            # split an episode images and labels into shots and query set\n",
    "            # note! data_shot shape is ( nway * kshot, 3, h, w ) not ( kshot * nway, 3, h, w )\n",
    "            # Take care when reshape the data shot\n",
    "            data_shot, data_query = data[:k], data[k:]\n",
    "\n",
    "            label_shot, label_query = label[:k], label[k:]\n",
    "            label_shot = sorted(list(set(label_shot.tolist())))\n",
    "\n",
    "            # convert labels into 0-4 values\n",
    "            label_query = label_query.tolist()\n",
    "            labels = []\n",
    "            for j in range(len(label_query)):\n",
    "                label = label_shot.index(label_query[j])\n",
    "                labels.append(label)\n",
    "            labels = torch.tensor(labels).cuda()\n",
    "            \n",
    "            if i==0 :\n",
    "                img_grid = torchvision.utils.make_grid(data_shot)\n",
    "                writer.add_image('data_shot', img_grid)\n",
    "                \n",
    "                \n",
    "            \"\"\" TODO 2 ( Same as above TODO 2 ) \"\"\"\n",
    "            \"\"\" Train the model \n",
    "            Input:\n",
    "                data_shot : torch.tensor, shot images, [args.nway * args.kshot, 3, h, w]\n",
    "                            be careful when using torch.reshape or .view functions\n",
    "                data_query : torch.tensor, query images, [args.query, 3, h, w]\n",
    "                labels : torch.tensor, labels of query images, [args.query]\n",
    "            output:\n",
    "                loss : torch scalar tensor which used for updating your model\n",
    "                logits : A value to measure accuracy and loss\n",
    "            \"\"\"\n",
    "            emb_shot = model(data_shot)\n",
    "            emb_query = model(data_query)\n",
    "\n",
    "            kshot = args.kshot\n",
    "            nway = args.nway\n",
    "            emb_dim = emb_shot.shape[-1]\n",
    "            n_query = labels.size()[0]\n",
    "\n",
    "            prototypes = torch.zeros((args.nway, emb_dim), requires_grad=True).cuda()\n",
    "            for nw in range(nway):\n",
    "                proto = emb_shot[nw*kshot :(nw+1)*kshot]\n",
    "                proto = torch.sum(proto, dim=0)/kshot\n",
    "                prototypes[nw,:] = proto\n",
    "\n",
    "            def _euc_dist(pro,q):\n",
    "                a = pro.shape[0]\n",
    "                logit = pro - q.unsqueeze(0).expand(a,-1)\n",
    "                logit = (logit**2).mean(1)\n",
    "                return logit.squeeze()\n",
    "            \n",
    "            logits = torch.zeros((n_query, nway), requires_grad=True).cuda()\n",
    "            for q in range(n_query):\n",
    "                logit = _euc_dist(prototypes, emb_query[q])\n",
    "                logits[q,:] = logit\n",
    "\n",
    "            loss = F.cross_entropy(-logits, labels)\n",
    "            \n",
    "\n",
    "            \"\"\" TODO 2 END \"\"\"\n",
    "            \n",
    "           \n",
    "            acc = count_acc(logits, labels)\n",
    "\n",
    "            tl.add(loss.item())\n",
    "            ta.add(acc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if (i+1) % PRINT_FREQ == 0:\n",
    "                #print(torch.argmin(logits, dim=1))\n",
    "            \n",
    "            proto = None; logits = None; loss = None\n",
    "            \n",
    "        \n",
    "        if (i+1) % PRINT_FREQ == 0:\n",
    "            writer.add_scalar('train loss', tl.item(), i+1)\n",
    "            writer.add_scalar('train acc', ta.item(), i+1)\n",
    "            \n",
    "            print('train {}, loss={:.4f} acc={:.4f}'.format(i+1, tl.item(), ta.item()))\n",
    "            # initialize loss and accuracy mean\n",
    "            tl = None\n",
    "            ta = None\n",
    "            tl = Averager()\n",
    "            ta = Averager()\n",
    "\n",
    "        # validation start\n",
    "        if (i+1) % VAL_FREQ == 0:\n",
    "            print('validation start')\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                vl = Averager()  # save average loss\n",
    "                va = Averager()  # save average accuracy\n",
    "                for j in range(VAL_TOTAL):\n",
    "                    for episode in val_data_loader:\n",
    "                        data, label = [_.cuda() for _ in episode]\n",
    "\n",
    "                        data_shot, data_query = data[:k], data[k:] # load an episode\n",
    "\n",
    "                        label_shot, label_query = label[:k], label[k:]\n",
    "                        label_shot = sorted(list(set(label_shot.tolist())))\n",
    "\n",
    "                        label_query = label_query.tolist()\n",
    "\n",
    "                        labels = []\n",
    "                        for j in range(len(label_query)):\n",
    "                            label = label_shot.index(label_query[j])\n",
    "                            labels.append(label)\n",
    "                        labels = torch.tensor(labels).cuda()\n",
    "\n",
    "                        \"\"\" TODO 2 ( Same as above TODO 2 ) \"\"\"\n",
    "                        \"\"\" Train the model \n",
    "                        Input:\n",
    "                            data_shot : torch.tensor, shot images, [args.nway * args.kshot, 3, h, w]\n",
    "                                        be careful when using torch.reshape or .view functions\n",
    "                            data_query : torch.tensor, query images, [args.query, 3, h, w]\n",
    "                            labels : torch.tensor, labels of query images, [args.query]\n",
    "                        output:\n",
    "                            loss : torch scalar tensor which used for updating your model\n",
    "                            logits : A value to measure accuracy and loss\n",
    "                        \"\"\"\n",
    "                        emb_shot = model(data_shot)\n",
    "                        emb_query = model(data_query)\n",
    "\n",
    "                        kshot = args.kshot\n",
    "                        nway = args.nway\n",
    "                        emb_dim = emb_shot.shape[-1]\n",
    "                        n_query = labels.size()[0]\n",
    "\n",
    "                        prototypes = torch.zeros((args.nway, emb_dim), requires_grad=True).cuda()\n",
    "                        for nw in range(nway):\n",
    "                            proto = emb_shot[nw*kshot :(nw+1)*kshot]\n",
    "                            proto = torch.sum(proto, dim=0)/kshot\n",
    "                            prototypes[nw,:] = proto\n",
    "\n",
    "                        def _euc_dist(pro,q):\n",
    "                            a = pro.shape[0]\n",
    "                            logit = pro - q.unsqueeze(0).expand(a,-1)\n",
    "                            logit = (logit**2).mean(1)\n",
    "                            return logit.squeeze()\n",
    "\n",
    "                        logits = torch.zeros((n_query, nway), requires_grad=True).cuda()\n",
    "                        for q in range(n_query):\n",
    "                            logit = _euc_dist(prototypes, emb_query[q])\n",
    "                            logits[q,:] = logit\n",
    "\n",
    "                        loss = F.cross_entropy(-logits, labels)\n",
    "\n",
    "\n",
    "                        \"\"\" TODO 2 END \"\"\"\n",
    "\n",
    "                        acc = count_acc(logits,labels)\n",
    "\n",
    "                        vl.add(loss.item())\n",
    "                        va.add(acc)\n",
    "\n",
    "                        proto = None; logits = None; loss = None\n",
    "                \n",
    "                writer.add_scalar('val loss', vl.item(), i+1)\n",
    "                writer.add_scalar('val acc',  va.item(), i+1)\n",
    "\n",
    "                print('val accuracy mean : %.4f' % va.item())\n",
    "                print('val loss mean : %.4f' % vl.item())\n",
    "\n",
    "                # initialize loss and accuracy mean\n",
    "                vl = None\n",
    "                va = None\n",
    "                vl = Averager()\n",
    "                va = Averager()\n",
    "            model.train()\n",
    "\n",
    "        if (i+1) % SAVE_FREQ == 0:\n",
    "            PATH =  args.ckpt_path + '/%06d_%s.pth' % (i + 1, args.name)\n",
    "            state = {'epoch': i , 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'scheduler': scheduler}\n",
    "            torch.save(state, PATH)\n",
    "            print('model saved, iteration : %d' % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.name = 'model'\n",
    "        self.dpath = './dataset/CUB_200_2011/CUB_200_2011'\n",
    "        self.restore_ckpt = True\n",
    "        self.nway = 5\n",
    "        self.kshot = 5\n",
    "        self.query = 20\n",
    "        self.ntest = 100\n",
    "        self.gpus = 0\n",
    "        self.exp_name = 'exp7'\n",
    "        self.ckpt_path = 'checkpoints/' + self.exp_name \n",
    "        self.writer_path = 'runs/' + self.exp_name \n",
    "\n",
    "    \n",
    "    def set_exp_name(self, name) : \n",
    "        self.exp_name = name\n",
    "        self.ckpt_path = 'checkpoints/' + name\n",
    "        self.writer_path = 'runs/' + name\n",
    "        \n",
    "args = args()\n",
    "\n",
    "    \n",
    "exp_name = input(\"Experiment name : \")\n",
    "args.set_exp_name(exp_name)\n",
    "is_restore = input(\"restore ? 1/0 : \")\n",
    "args.restore_ckpt = int(is_restore)\n",
    "\n",
    "if not os.path.isdir(args.ckpt_path):\n",
    "    os.mkdir(args.ckpt_path)\n",
    "    \n",
    "torch.cuda.set_device(args.gpus)\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train dataset mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3882586399353469"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()*()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(args.dpath, state='train')\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "mean_t = 0.\n",
    "std_t = 0.\n",
    "nb_samples_t = 0.\n",
    "\n",
    "for data in loader:\n",
    "    batch_samples = data[0].size(0)\n",
    "    data = data[0].view(batch_samples, data[0].size(1), -1)\n",
    "    mean_t += data.mean(2).sum(0)\n",
    "    std_t += data.std(2).sum(0)\n",
    "    nb_samples_t += batch_samples\n",
    "\n",
    "mean_t /= nb_samples_t\n",
    "std_t /= nb_samples_t\n",
    "print(mean_t, std_t, nb_samples_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val dataset mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(args.dpath, state='val')\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "mean_v = 0.\n",
    "std_v = 0.\n",
    "nb_samples_v= 0.\n",
    "\n",
    "for data in loader:\n",
    "    batch_samples = data[0].size(0)\n",
    "    data = data[0].view(batch_samples, data[0].size(1), -1)\n",
    "    mean_v += data.mean(2).sum(0)\n",
    "    std_v += data.std(2).sum(0)\n",
    "    nb_samples_v += batch_samples\n",
    "\n",
    "mean_v /= nb_samples_v\n",
    "std_v /= nb_samples_v\n",
    "print(mean_v, std_v, nb_samples_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((mean_v * nb_samples_v) + (mean_t * nb_samples_t))/(nb_samples_t+nb_samples_v),\n",
    "((std_v * nb_samples_v) + (std_t * nb_samples_t))/(nb_samples_t+nb_samples_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(emb_shot, emb_query, labels):\n",
    "        kshot = args.kshot\n",
    "        nway = args.nway\n",
    "        emb_dim = emb_shot.shape[-1]\n",
    "        n_query = labels.size()[0]\n",
    "\n",
    "        prototypes = torch.zeros((args.nway, emb_dim), requires_grad=True).cuda()\n",
    "        for i in range(nway):\n",
    "            proto = emb_shot[i*kshot :(i+1)*kshot]\n",
    "            proto = torch.sum(proto, dim=0)/kshot\n",
    "            prototypes[i,:] = proto\n",
    "\n",
    "        def _euc_dist(pro,q):\n",
    "            a = pro.shape[0]\n",
    "            logit = pro - q.unsqueeze(0).expand(a,-1)\n",
    "            logit = (logit**2).sum(1)\n",
    "            return logit.squeeze()\n",
    "\n",
    "\n",
    "        logits = torch.zeros((n_query, nway), requires_grad=True).cuda()\n",
    "        for i in range(n_query):\n",
    "            logit = _euc_dist(prototypes, emb_query[i])\n",
    "            logits[i,:] = logit\n",
    "\n",
    "        loss = F.cross_entropy(-logits, labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(emb_shot, emb_query, labels):\n",
    "    prototypes = []\n",
    "    kshot = args.kshot\n",
    "    for i in range(args.nway):\n",
    "        proto = emb_shot[i*kshot :(i+1)*kshot]\n",
    "        proto = torch.sum(proto, dim=0)/kshot\n",
    "        prototypes.append(proto.tolist())\n",
    "    prototypes = torch.tensor(prototypes).cuda()\n",
    "\n",
    "    def _euc_dist(pro,q):\n",
    "        a = pro.shape[0]\n",
    "        logits = pro - q.unsqueeze(0).expand(a,-1)\n",
    "        logits = (logits**2).sum(1)\n",
    "        return logits.squeeze()\n",
    "\n",
    "    def _one_hot(labels):\n",
    "        one_hot = torch.zeros(labels.size()[0], args.nway)\n",
    "        one_hot[torch.arange(labels.size()[0]), labels] =1\n",
    "        return one_hot\n",
    "\n",
    "    loss = 0\n",
    "    logits = []\n",
    "    n_query = labels.size()[0]\n",
    "    one_hot = _one_hot(labels)\n",
    "    for i in range(n_query):\n",
    "        logit = _euc_dist(prototypes, emb_query[i]).cuda()\n",
    "        logits.append(logit)\n",
    "        loss = loss + F.cross_entropy(logit.transpose(0,1), one_hot[i]).item()\n",
    "\n",
    "    loss = loss/n_query\n",
    "    logits = torch.tensor(logits).cuda()\n",
    "\n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tl = Averager()  # save average loss\n",
    "ta = Averager()  # save average accuracy\n",
    "# training start\n",
    "print('train start')\n",
    "for i in range(TOTAL):\n",
    "    for episode in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        print('iteration', i)\n",
    "        if torch.cuda.is_available():\n",
    "            data, label = [_.cuda() for _ in episode]  # load an episode\n",
    "        # split an episode images and labels into shots and query set\n",
    "        # note! data_shot shape is ( nway * kshot, 3, h, w ) not ( kshot * nway, 3, h, w )\n",
    "        # Take care when reshape the data shot\n",
    "        data_shot, data_query = data[:k], data[k:]\n",
    "\n",
    "        label_shot, label_query = label[:k], label[k:]\n",
    "        label_shot = sorted(list(set(label_shot.tolist())))\n",
    "\n",
    "        # convert labels into 0-4 values\n",
    "        label_query = label_query.tolist()\n",
    "        labels = []\n",
    "        for j in range(len(label_query)):\n",
    "            label = label_shot.index(label_query[j])\n",
    "            labels.append(label)\n",
    "        labels = torch.tensor(labels).cuda()\n",
    "\n",
    "        \"\"\" TODO 2 ( Same as above TODO 2 ) \"\"\"\n",
    "        \"\"\" Train the model \n",
    "        Input:\n",
    "            data_shot : torch.tensor, shot images, [args.nway * args.kshot, 3, h, w]\n",
    "                        be careful when using torch.reshape or .view functions\n",
    "            data_query : torch.tensor, query images, [args.query, 3, h, w]\n",
    "            labels : torch.tensor, labels of query images, [args.query]\n",
    "        output:\n",
    "            loss : torch scalar tensor which used for updating your model\n",
    "            logits : A value to measure accuracy and loss\n",
    "        \"\"\"\n",
    "        emb_shot = model(data_shot)\n",
    "        emb_query = model(data_query)\n",
    "        loss, logit = loss_fn(emb_shot, emb_query, labels)\n",
    "        \"\"\" TODO 2 END \"\"\"\n",
    "        acc = count_acc(logit, labels)\n",
    "\n",
    "        tl.add(loss.item())\n",
    "        ta.add(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        proto = None; logits = None; loss = None\n",
    "\n",
    "    if (i+1) % PRINT_FREQ == 0:\n",
    "        print('train {}, loss={:.4f} acc={:.4f}'.format(i+1, tl.item(), ta.item()))\n",
    "\n",
    "        # initialize loss and accuracy mean\n",
    "        tl = None\n",
    "        ta = None\n",
    "        tl = Averager()\n",
    "        ta = Averager()\n",
    "\n",
    "    # validation start\n",
    "    if (i+1) % VAL_FREQ == 0:\n",
    "        print('validation start')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vl = Averager()  # save average loss\n",
    "            va = Averager()  # save average accuracy\n",
    "            for j in range(VAL_TOTAL):\n",
    "                for episode in val_data_loader:\n",
    "                    data, label = [_.cuda() for _ in episode]\n",
    "\n",
    "                    data_shot, data_query = data[:k], data[k:] # load an episode\n",
    "\n",
    "                    label_shot, label_query = label[:k], label[k:]\n",
    "                    label_shot = sorted(list(set(label_shot.tolist())))\n",
    "\n",
    "                    label_query = label_query.tolist()\n",
    "\n",
    "                    labels = []\n",
    "                    for j in range(len(label_query)):\n",
    "                        label = label_shot.index(label_query[j])\n",
    "                        labels.append(label)\n",
    "                    labels = torch.tensor(labels).cuda()\n",
    "\n",
    "                    \"\"\" TODO 2 ( Same as above TODO 2 ) \"\"\"\n",
    "                    \"\"\" Train the model \n",
    "                    Input:\n",
    "                        data_shot : torch.tensor, shot images, [args.nway * args.kshot, 3, h, w]\n",
    "                                    be careful when using torch.reshape or .view functions\n",
    "                        data_query : torch.tensor, query images, [args.query, 3, h, w]\n",
    "                        labels : torch.tensor, labels of query images, [args.query]\n",
    "                    output:\n",
    "                        loss : torch scalar tensor which used for updating your model\n",
    "                        logits : A value to measure accuracy and loss\n",
    "                    \"\"\"\n",
    "\n",
    "                    emb_shot = model(data_shot)\n",
    "                    emb_query = model(data_query)\n",
    "                    loss, logit = loss_fn(data_shot, data_query, labels)\n",
    "\n",
    "                    \"\"\" TODO 2 END \"\"\"\n",
    "\n",
    "                    acc = count_acc(logit,labels)\n",
    "\n",
    "                    vl.add(loss.item())\n",
    "                    va.add(acc)\n",
    "\n",
    "                    proto = None; logits = None; loss = None\n",
    "\n",
    "            print('val accuracy mean : %.4f' % va.item())\n",
    "            print('val loss mean : %.4f' % vl.item())\n",
    "\n",
    "            # initialize loss and accuracy mean\n",
    "            vl = None\n",
    "            va = None\n",
    "            vl = Averager()\n",
    "            va = Averager()\n",
    "        model.train()\n",
    "\n",
    "    if (i+1) % SAVE_FREQ == 0:\n",
    "        PATH = 'checkpoints/%d_%s.pth' % (i + 1, args.name)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        print('model saved, iteration : %d' % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([torch.tensor([1,2,3,4,5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([torch.tensor([1,2,3,4,5]).tolist(),torch.tensor([1,2,3,4,6]).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--name', default='model', help=\"name your experiment\")\n",
    "parser.add_argument('--dpath', '--d', default='./dataset/CUB_200_2011/CUB_200_2011', type=str,\n",
    "                    help='the path where dataset is located')\n",
    "parser.add_argument('--restore_ckpt', type=str, help=\"restore checkpoint\")\n",
    "parser.add_argument('--nway', '--n', default=5, type=int, help='number of class in the support set (5 or 20)')\n",
    "parser.add_argument('--kshot', '--k', default=5, type=int,\n",
    "                    help='number of data in each class in the support set (1 or 5)')\n",
    "parser.add_argument('--query', '--q', default=20, type=int, help='number of query data')\n",
    "parser.add_argument('--ntest', default=100, type=int, help='number of tests')\n",
    "parser.add_argument('--gpus', type=int, nargs='+', default=1)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
